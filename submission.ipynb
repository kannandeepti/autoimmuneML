{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade crunch-cli\n",
    "!crunch setup --notebook broad-1 aecv1 --token fvRWX6iwSOecF2ZdjxFeSxiZrSj0Cq6T0fahLJ12lkn1WhbysYagOW3c5NTeFlTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.26.4 pandas torch scipy openslide-python pydantic pytorch-lightning dask distributed matplotlib seaborn scikit-learn opencv-python scanpy spatialdata zarr ome-zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Core Python Libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import joblib\n",
    "from types import SimpleNamespace\n",
    "from operator import itemgetter\n",
    "from abc import abstractmethod\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Core Data manipulation Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, ConstantInputWarning\n",
    "\n",
    "# Visualization Library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Spatial Data Processing\n",
    "import spatialdata as sd  # Manage multi-modal spatial omics datasets\n",
    "import anndata as ad  # Manage annotated data matrices in memory and on disk\n",
    "import scanpy as sc  # For analyzing single-cell data, especially for dimensionality reduction and clustering.\n",
    "from skimage.measure import regionprops  # Get region properties of nucleus/cell image from masked nucleus image\n",
    "import h5py  # For handling HDF5 data files\n",
    "\n",
    "# Frameworks for ML and DL models\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import timm  # timm: A library to load pretrained SOTA computer vision models (e.g. classification, feature extraction, ...)\n",
    "from sklearn.linear_model import Ridge  # Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hdf5(output_fpath, asset_dict, attr_dict=None, mode='a', auto_chunk=True, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Save data and attributes into an HDF5 file, or initialize a new file with the given data.\n",
    "\n",
    "    Parameters:\n",
    "        output_fpath (str): Path to save the HDF5 file.\n",
    "        asset_dict (dict): Dictionary containing keys and their corresponding data (e.g., numpy arrays) to save.\n",
    "        attr_dict (dict, optional): Dictionary of attributes for each key. Format: {key: {attr_key: attr_val, ...}}.\n",
    "        mode (str): File mode ('a' for append, 'w' for write, etc.).\n",
    "        auto_chunk (bool): Whether to enable automatic chunking for HDF5 datasets.\n",
    "        chunk_size (int, optional): If auto_chunk is False, specify the chunk size for the first dimension.\n",
    "\n",
    "    Returns:\n",
    "        str: Path of the saved HDF5 file.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(output_fpath, mode) as f:\n",
    "        for key, val in asset_dict.items():\n",
    "            data_shape = val.shape\n",
    "            # Ensure data has at least 2 dimensions\n",
    "            if len(data_shape) == 1:\n",
    "                val = np.expand_dims(val, axis=1)\n",
    "                data_shape = val.shape\n",
    "\n",
    "            if key not in f:  # if key does not exist, create a new dataset\n",
    "                data_type = val.dtype\n",
    "\n",
    "                if data_type.kind == 'U':  # Handle Unicode strings\n",
    "                    chunks = (1, 1)\n",
    "                    max_shape = (None, 1)\n",
    "                    data_type = h5py.string_dtype(encoding='utf-8')\n",
    "                else:\n",
    "                    if data_type == np.object_:\n",
    "                        data_type = h5py.string_dtype(encoding='utf-8')\n",
    "                    # Determine chunking strategy\n",
    "                    if auto_chunk:\n",
    "                        chunks = True  # let h5py decide chunk size\n",
    "                    else:\n",
    "                        chunks = (chunk_size,) + data_shape[1:]\n",
    "                    maxshape = (None,) + data_shape[1:]  # Allow unlimited size for the first dimension\n",
    "\n",
    "                try:\n",
    "                    dset = f.create_dataset(key,\n",
    "                                            shape=data_shape,\n",
    "                                            chunks=chunks,\n",
    "                                            maxshape=maxshape,\n",
    "                                            dtype=data_type)\n",
    "                    # Save attributes for the dataset\n",
    "                    if attr_dict is not None:\n",
    "                        if key in attr_dict.keys():\n",
    "                            for attr_key, attr_val in attr_dict[key].items():\n",
    "                                dset.attrs[attr_key] = attr_val\n",
    "                    # Write the data to the dataset\n",
    "                    dset[:] = val\n",
    "                except:\n",
    "                    print(f\"Error encoding {key} of dtype {data_type} into hdf5\")\n",
    "\n",
    "            else:  # Append data to an existing dataset\n",
    "                dset = f[key]\n",
    "                dset.resize(len(dset) + data_shape[0], axis=0)\n",
    "                # assert dset.dtype == val.dtype\n",
    "                dset[-data_shape[0]:] = val\n",
    "\n",
    "    return output_fpath\n",
    "\n",
    "\n",
    "def read_assets_from_h5(h5_path, keys=None, skip_attrs=False, skip_assets=False):\n",
    "    \"\"\"\n",
    "    Read data and attributes from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "        h5_path (str): Path to the HDF5 file.\n",
    "        keys (list, optional): List of keys to read. Reads all keys if None.\n",
    "        skip_attrs (bool): If True, skip reading attributes.\n",
    "        skip_assets (bool): If True, skip reading data assets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary of data assets and a dictionary of attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    assets = {}\n",
    "    attrs = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        if keys is None:\n",
    "            keys = list(f.keys())\n",
    "\n",
    "        for key in keys:\n",
    "            if not skip_assets:\n",
    "                assets[key] = f[key][:]\n",
    "            if not skip_attrs and f[key].attrs is not None:\n",
    "                attrs[key] = dict(f[key].attrs)\n",
    "\n",
    "    return assets, attrs\n",
    "\n",
    "\n",
    "class Patcher:\n",
    "    def __init__(self, image, coords, patch_size_target, name=None):\n",
    "        \"\"\"\n",
    "        Initializes the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
    "\n",
    "        :param image: Input image as a numpy array (H x W x 3), the input image from which patches will be extracted.\n",
    "        :param coords: List or array of cell coordinates (centroÃ¯d) [(x1, y1), (x2, y2), ...].\n",
    "        :param patch_size_target: Target size of patches.\n",
    "        :param name: Name of the whole slide image (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        self.image = image\n",
    "        self.height, self.width = image.shape[:2]\n",
    "        self.coords = coords\n",
    "        self.patch_size_target = patch_size_target\n",
    "        self.name = name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over coordinates, yielding image patches and their coordinates.\n",
    "        \"\"\"\n",
    "\n",
    "        for x, y in self.coords:\n",
    "            # Extract patch dimension centered at (x, y)\n",
    "            x_start = max(x - self.patch_size_target // 2, 0)\n",
    "            y_start = max(y - self.patch_size_target // 2, 0)\n",
    "            x_end = min(x_start + self.patch_size_target, self.width)\n",
    "            y_end = min(y_start + self.patch_size_target, self.height)\n",
    "\n",
    "            # Ensure the patch size matches the target size, padding with zeros if necessary\n",
    "            patch = np.zeros((self.patch_size_target, self.patch_size_target, 3), dtype=np.uint8)\n",
    "            patch[:y_end - y_start, :x_end - x_start, :] = self.image[y_start:y_end, x_start:x_end, :]\n",
    "\n",
    "            yield patch, x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of patches based on the number of coordinates.\n",
    "        This is used to determine how many iterations will be done when iterating over the object.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.coords)\n",
    "\n",
    "    def save_visualization(self, path, vis_width=300, dpi=150):\n",
    "        \"\"\"\n",
    "        Save a visualization of patches overlayed on the tissue H&E image.\n",
    "        This function creates a plot where each patch's location is marked with a rectangle overlaid on the image.\n",
    "\n",
    "        :param path: File path where the visualization will be saved.\n",
    "        :param vis_width: Target width of the visualization in pixels.\n",
    "        :param dpi: Resolution of the saved visualization.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the tissue visualization mask\n",
    "        mask_plot = self.image\n",
    "\n",
    "        # Calculate downscale factor for visualization\n",
    "        downscale_vis = vis_width / self.width\n",
    "\n",
    "        # Create a plot\n",
    "        _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
    "        ax.imshow(mask_plot)\n",
    "\n",
    "        # Add patches\n",
    "        patch_rectangles = []\n",
    "        for x, y in self.coords:\n",
    "            x_start, y_start = x - self.patch_size_target // 2, y - self.patch_size_target // 2\n",
    "            patch_rectangles.append(Rectangle((x_start, y_start), self.patch_size_target, self.patch_size_target))\n",
    "\n",
    "        # Add rectangles to the plot\n",
    "        ax.add_collection(PatchCollection(patch_rectangles, facecolor='none', edgecolor='black', linewidth=0.3))\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def view_coord_points(self, vis_width=300, dpi=150):\n",
    "        \"\"\"\n",
    "        Visualizes the coordinates as small points in 2D.\n",
    "        This function generates a scatter plot of the patch coordinates on the H&E image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate downscale factor for visualization\n",
    "        downscale_vis = vis_width / self.width\n",
    "\n",
    "        # Create a plot\n",
    "        # _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
    "        # plt.scatter(self.coords[:, 0], -self.coords[:, 1], s=0.2)\n",
    "        # plt.show()\n",
    "        # plt.close()\n",
    "\n",
    "    def to_h5(self, path, extra_assets={}):\n",
    "        \"\"\"\n",
    "        Saves the extracted patches and their associated information to an HDF5 file.\n",
    "\n",
    "        Each patch is saved as a dataset along with its coordinates and any additional assets (extra_assets).\n",
    "        The HDF5 file is structured with a dataset for the image patch ('img') and coordinates ('coords').\n",
    "\n",
    "        :param path: File path where the HDF5 file will be saved.\n",
    "        :param extra_assets: Dictionary of additional assets to save (optional). Each value in extra_assets must have the same length as the patches.\n",
    "        \"\"\"\n",
    "\n",
    "        mode_HE = 'w'  # Start with write mode for the first patch\n",
    "        i = 0\n",
    "\n",
    "        # Check that the extra_assets match the number of patches\n",
    "        if extra_assets:\n",
    "            for _, value in extra_assets.items():\n",
    "                if len(value) != len(self):\n",
    "                    raise ValueError(\"Each value in extra_assets must have the same length as the patcher object.\")\n",
    "\n",
    "        # Ensure the file has the correct extension\n",
    "        if not (path.endswith('.h5') or path.endswith('.h5ad')):\n",
    "            path = path + '.h5'\n",
    "\n",
    "        # Loop through each patch and save it to the HDF5 file (loop through __iter__ function)\n",
    "        for tile, x, y in tqdm(self):\n",
    "            assert tile.shape == (self.patch_size_target, self.patch_size_target, 3)\n",
    "\n",
    "            # Prepare the data to be saved for this patch\n",
    "            asset_dict = {\n",
    "                'img': np.expand_dims(tile, axis=0),  # Shape (1, h, w, 3)\n",
    "                'coords': np.expand_dims([x, y], axis=0)  # Shape (1, 2)\n",
    "            }\n",
    "\n",
    "            # Add any extra assets to the asset dictionary\n",
    "            extra_asset_dict = {key: np.expand_dims([value[i]], axis=0) for key, value in extra_assets.items()}\n",
    "            asset_dict = {**asset_dict, **extra_asset_dict}\n",
    "\n",
    "            # Define the attributes for the image patch\n",
    "            attr_dict = {'img': {'patch_size_target': self.patch_size_target}}\n",
    "\n",
    "            if self.name is not None:\n",
    "                attr_dict['img']['name'] = self.name\n",
    "\n",
    "            # Save the patch data to the HDF5 file\n",
    "            save_hdf5(path, asset_dict, attr_dict, mode=mode_HE, auto_chunk=False, chunk_size=1)\n",
    "            mode_HE = 'a'  # Switch to append mode after the first patch\n",
    "            i += 1\n",
    "\n",
    "def extract_spatial_positions(sdata, cell_id_list):\n",
    "    \"\"\"\n",
    "    Extracts spatial positions (centroids) of regions from the nucleus image where cell IDs match the provided cell list.\n",
    "\n",
    "    Need to use 'HE_nuc_original' to extract spatial coordinate of cells\n",
    "    HE_nuc_original: The nucleus segmentation mask of H&E image, in H&E native coordinate system. The cell_id in this segmentation mask matches with the nuclei by gene matrix stored in anucleus.\n",
    "    HE_nuc_original is like a binary segmentation mask 0 - 1 but replace 1 with cell_ids.\n",
    "    You can directly find the location of a cell, with cell_id, through HE_nuc_original==cell_id\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the nucleus segmentation mask ('HE_nuc_original').\n",
    "    cell_id_list: array-like\n",
    "        A list or array of cell IDs to filter the regions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        A NumPy array of spatial coordinates (x_center, y_center) for matched regions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Extracting spatial positions ...\")\n",
    "    # Get region properties from the nucleus image: for each cell_id get its location on HE image\n",
    "    regions = regionprops(sdata['HE_nuc_original'][0, :, :].to_numpy())\n",
    "\n",
    "    dict_spatial_positions = {}\n",
    "    # Loop through each region and extract centroid if the cell ID matches\n",
    "    for props in tqdm(regions):\n",
    "        cell_id = props.label\n",
    "        centroid = props.centroid\n",
    "        # Extract only coordinates from the provided cell_id list\n",
    "        if cell_id in cell_id_list:\n",
    "            y_center, x_center = int(centroid[0]), int(centroid[1])\n",
    "            dict_spatial_positions[cell_id] = [x_center, y_center]\n",
    "\n",
    "    # To maintain cell IDs order\n",
    "    spatial_positions = []\n",
    "    for cell_id in cell_id_list:\n",
    "        try:\n",
    "            spatial_positions.append(dict_spatial_positions[cell_id])\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Cell ID {cell_id} not found in the segmentation mask.\")\n",
    "            spatial_positions.append([1000, 1000])\n",
    "\n",
    "    return np.array(spatial_positions)\n",
    "\n",
    "\n",
    "def process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
    "                                show_extracted_images=False, vis_width=1000):\n",
    "    \"\"\"\n",
    "    Load and process the spatial image data, creates patches, saves them in an HDF5 file,\n",
    "    and visualizes the extracted images and spatial coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
    "    patch_save_dir: str\n",
    "        Directory where the resulting HDF5 file and visualizations will be saved.\n",
    "    name_data: str\n",
    "        Name used for saving the dataset.\n",
    "    coords_center: array-like\n",
    "        Coordinates of the regions to be patched (centroids of cell regions).\n",
    "    target_patch_size: int\n",
    "        Size of the patches to extract from the image.\n",
    "    barcodes: array-like\n",
    "        Barcodes associated with patches.\n",
    "    show_extracted_images: bool, optional (default=False)\n",
    "        If True, will show extracted images during the visualization phase.\n",
    "    vis_width: int, optional (default=1000)\n",
    "        Width of the visualization images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image and transpose it to the correct format\n",
    "    print(\"Loading imgs ...\")\n",
    "    intensity_image = np.transpose(sdata['HE_original'].to_numpy(), (1, 2, 0))\n",
    "\n",
    "    print(\"Patching: create image dataset (X) ...\")\n",
    "    # Path for the .h5 image dataset\n",
    "    h5_path = os.path.join(patch_save_dir, name_data + '.h5')\n",
    "\n",
    "    # Create the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
    "    patcher = Patcher(\n",
    "        image=intensity_image,\n",
    "        coords=coords_center,\n",
    "        patch_size_target=target_patch_size\n",
    "    )\n",
    "\n",
    "    # Build and Save patches to an HDF5 file\n",
    "    patcher.to_h5(h5_path, extra_assets={'barcode': barcodes})\n",
    "\n",
    "    # Visualization\n",
    "    print(\"Visualization\")\n",
    "    if show_extracted_images:\n",
    "        print(\"Extracted Images (high time and memory consumption...)\")\n",
    "        patcher.save_visualization(os.path.join(patch_save_dir, name_data + '_viz.png'), vis_width=vis_width)\n",
    "\n",
    "    print(\"Spatial coordinates\")\n",
    "    patcher.view_coord_points(vis_width=vis_width)\n",
    "\n",
    "    # Display some example images from the created dataset\n",
    "    print(\"Examples from the created .h5 dataset\")\n",
    "    assets, _ = read_assets_from_h5(h5_path)\n",
    "\n",
    "    n_images = 3\n",
    "    # fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
    "    # for i in range(n_images):\n",
    "    #     axes[i].imshow(assets[\"img\"][i])\n",
    "    # for ax in axes:\n",
    "    #     ax.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    # Delete variables that are no longer used\n",
    "    del intensity_image, patcher, assets\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def preprocess_spatial_transcriptomics_data_train(list_ST_name_data, data_directory_path, dir_processed_dataset, size_subset=None, target_patch_size=32, vis_width=1000, show_extracted_images=False):\n",
    "    \"\"\"\n",
    "    Train step: Preprocesses spatial transcriptomics data by performing the following steps for each ST:\n",
    "    1. Samples the dataset and extract spatial coordinates of cells.\n",
    "    2. Extract gene expression data (Y) and save it as `.h5ad` files into directory 'adata'.\n",
    "    4. Generates and saves patches of images centered on spatial coordinates to HDF5 files (X) into directory 'patches'.\n",
    "    5. Saves the list of genes to a JSON file into direcotry 'splits'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    list_ST_name_data: list\n",
    "        List of spatial transcriptomics data names.\n",
    "    data_directory_path: str\n",
    "        Path to the directory containing the input data in `.zarr` format.\n",
    "    dir_processed_dataset: str\n",
    "        Path to the directory where processed datasets and outputs will be saved.\n",
    "    size_subset: int, optional\n",
    "        ST data sample size. If None, no sampling.\n",
    "    target_patch_size: int, optional\n",
    "        Target size of image patches to extract.\n",
    "    vis_width: int, optional\n",
    "        Width of the visualization output for spatial and image patches.\n",
    "    show_extracted_images: bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates directories for saving patches (X) ('patches'), processed AnnData objects (Y) ('adata'), and train/test dataset splits ('splits').\n",
    "    patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
    "    adata_save_dir = os.path.join(dir_processed_dataset, \"adata\")\n",
    "    splits_save_dir = os.path.join(dir_processed_dataset, \"splits\")\n",
    "    os.makedirs(patch_save_dir, exist_ok=True)\n",
    "    os.makedirs(adata_save_dir, exist_ok=True)\n",
    "    os.makedirs(splits_save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\\n\")\n",
    "\n",
    "    # Loop through each dataset name\n",
    "    for count, name_data in enumerate(list_ST_name_data):\n",
    "        print(f\"\\nDATA ({count+1}/{len(list_ST_name_data)}): {name_data}\\n\")\n",
    "\n",
    "        # Load the spatial transcriptomics data from the .zarr format\n",
    "        sdata = sd.read_zarr(os.path.join(data_directory_path, f\"{name_data}.zarr\"))\n",
    "\n",
    "        # Extract the list of gene names\n",
    "        gene_name_list = sdata['anucleus'].var['gene_symbols'].values\n",
    "\n",
    "        # Sample the dataset if a subset size is specified\n",
    "        if size_subset is not None:\n",
    "            print(\"Sampling the dataset ...\")\n",
    "            rows_to_keep = list(sdata['anucleus'].obs.sample(n=min(size_subset, len(sdata['anucleus'].obs)), random_state=42).index)\n",
    "        else:\n",
    "            size_subset = len(sdata['anucleus'].obs)\n",
    "            rows_to_keep = list(sdata['anucleus'].obs.sample(n=size_subset, random_state=42).index)\n",
    "\n",
    "        # Extract spatial positions for 'train' cells\n",
    "        cell_id_train = sdata['anucleus'].obs[\"cell_id\"].values\n",
    "        new_spatial_coord = extract_spatial_positions(sdata, cell_id_train)\n",
    "        # Store new spatial coordinates into sdata\n",
    "        sdata['anucleus'].obsm['spatial'] = new_spatial_coord\n",
    "\n",
    "        # Create the gene expression dataset (Y)\n",
    "        print(\"Create gene expression dataset (Y) ...\")\n",
    "        y_subtracted = sdata['anucleus'][rows_to_keep].copy()\n",
    "        # Trick to set all index to same length to avoid problems when saving to h5\n",
    "        y_subtracted.obs.index = ['x' + str(i).zfill(6) for i in y_subtracted.obs.index]\n",
    "\n",
    "        # Save the gene expression data to an H5AD file\n",
    "        y_subtracted.write(os.path.join(adata_save_dir, f'{name_data}.h5ad'))\n",
    "\n",
    "        for index in y_subtracted.obs.index:\n",
    "            if len(index) != len(y_subtracted.obs.index[0]):\n",
    "                warnings.warn(\"indices of y_subtracted.obs should all have the same length to avoid problems when saving to h5\", UserWarning)\n",
    "\n",
    "        # Extract spatial coordinates and barcodes (cell IDs) for the patches\n",
    "        coords_center = y_subtracted.obsm['spatial']\n",
    "        barcodes = np.array(y_subtracted.obs.index)\n",
    "\n",
    "        # Generate and visualize image patches centered around spatial coordinates ({name_data}.h5 file in directory os.path.join(dir_processed_dataset, \"patches\"))\n",
    "        process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
    "                                    show_extracted_images=False, vis_width=1000)\n",
    "\n",
    "        # Delete variables that are no longer used\n",
    "        del sdata, y_subtracted\n",
    "        gc.collect()\n",
    "\n",
    "    # Save the gene list to a JSON file\n",
    "    gene_path = os.path.join(dir_processed_dataset, 'var_genes.json')\n",
    "    print(f\"Save gene list in {gene_path}\")\n",
    "    data = {\n",
    "        \"genes\": list(gene_name_list)\n",
    "    }\n",
    "    print(\"Total number of genes:\", len(data[\"genes\"]))\n",
    "\n",
    "    with open(gene_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(\"\\nPreprocess dataset DONE:\", \" - \".join(list_ST_name_data), \"\\n\")\n",
    "\n",
    "\n",
    "def preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_id_list, dir_processed_dataset, target_patch_size=32, vis_width=1000, show_extracted_images=False):\n",
    "    \"\"\"\n",
    "    Test step: Preprocesses spatial transcriptomics data by performing the following steps for the selected ST data:\n",
    "    1. Extract spatial coordinates of the selected cells.\n",
    "    2. Generates and saves patches of images centered on spatial coordinates to HDF5 files (X) into directory 'patches'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name_data: str\n",
    "        Name used for saving the dataset.\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
    "    cell_id_list : array-like\n",
    "        A list or array of cell IDs to filter the regions.\n",
    "    dir_processed_dataset: str\n",
    "        Path to the directory where processed datasets and outputs will be saved.\n",
    "    target_patch_size: int, optional\n",
    "        Target size of image patches to extract.\n",
    "    vis_width: int, optional\n",
    "        Width of the visualization output for spatial and image patches.\n",
    "    show_extracted_images: bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates directories for saving patches ('patches')\n",
    "    patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
    "    os.makedirs(patch_save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\\n\")\n",
    "\n",
    "    # Extract spatial positions for selected cells\n",
    "    new_spatial_coord = extract_spatial_positions(sdata, cell_id_list)\n",
    "\n",
    "    # Spatial coordinates and barcodes (cell IDs) for the patches\n",
    "    coords_center = new_spatial_coord\n",
    "    barcodes = np.array(['x' + str(i).zfill(6) for i in list(cell_id_list)])  # Trick to set all index to same length to avoid problems when saving to h5\n",
    "\n",
    "    # Generate and visualize image patches centered around spatial coordinates ({name_data}.h5 file in directory os.path.join(dir_processed_dataset, \"patches\"))\n",
    "    process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
    "                                show_extracted_images=False, vis_width=1000)\n",
    "\n",
    "    print(\"\\nPreprocess dataset DONE\\n\")\n",
    "\n",
    "\n",
    "def create_cross_validation_splits(dir_processed_dataset, n_fold=None):\n",
    "    \"\"\"\n",
    "    Creates cross-validation splits (leave-one-out cv) for spatial transcriptomics data by splitting\n",
    "    samples into training and testing sets and saving them as CSV files.\n",
    "\n",
    "    Example for samples [\"UC1_NI\", \"UC1_I\", \"UC6_NI\"]:\n",
    "      FOLD 0: TRAIN: [\"UC1_NI\", \"UC1_I\"] TEST: [\"UC6_NI\"]\n",
    "      FOLD 1: TRAIN: [\"UC1_NI\", \"UC6_NI\"] TEST: [\"UC1_I\"]\n",
    "      FOLD 2: TRAIN: [\"UC6_NI\", \"UC1_I\"] TEST: [\"UC1_NI\"]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dir_processed_dataset : str\n",
    "        Path to the directory where processed datasets are saved.\n",
    "    n_fold : int, optional\n",
    "        Number of folds for cross-validation (leave-one-out cv). If None, defaults to number of ST files.\n",
    "    \"\"\"\n",
    "\n",
    "    patches_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
    "    splits_dir = os.path.join(dir_processed_dataset, \"splits\")\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "    # List all files in the patches directory (these represent individual samples)\n",
    "    patch_files = os.listdir(patches_dir)\n",
    "\n",
    "    # Prepare a list to store information about the samples (patches and gene expression data path)\n",
    "    all_ST = []\n",
    "    # Extra paths by iterating over patch files\n",
    "    for patch_file in patch_files:\n",
    "        if patch_file.endswith('.h5'):\n",
    "            # Extract sample ID from patch file name\n",
    "            sample_id = patch_file.split('.')[0]\n",
    "            # Corresponding gene expression data file (should be in 'adata' directory)\n",
    "            expr_file = os.path.join(\"adata\", f\"{sample_id}.h5ad\")\n",
    "            all_ST.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"patches_path\": os.path.join(\"patches\", patch_file),\n",
    "                \"expr_path\": expr_file\n",
    "            })\n",
    "\n",
    "    df_all_ST = pd.DataFrame(all_ST)\n",
    "\n",
    "    # If n_fold is not specified, default to using the number of available samples\n",
    "    if n_fold is None:\n",
    "        n_fold = len(df_all_ST)\n",
    "    # Ensure that the number of folds does not exceed the number of available samples\n",
    "    n_fold = min(n_fold, len(df_all_ST))\n",
    "\n",
    "    print(\"\\n -- CREATE CROSS-VALIDATION SPLITS --------------------------------------------\\n\")\n",
    "\n",
    "    # Generate cross-validation splits (leave-one-out CV)\n",
    "    for i in range(n_fold):\n",
    "        # Select the current sample as the test set (leave-one-out)\n",
    "        test_df = df_all_ST.iloc[[i]]\n",
    "        # Use the remaining samples as the training set\n",
    "        train_df = df_all_ST.drop(i)\n",
    "\n",
    "        print(f\"Index {i}:\")\n",
    "        print(\"Train DataFrame:\")\n",
    "        print(train_df)\n",
    "        print(\"Test DataFrame:\")\n",
    "        print(test_df)\n",
    "\n",
    "        # Save the train and test DataFrames as CSV files in the splits directory\n",
    "        train_filename = f\"train_{i}.csv\"\n",
    "        test_filename = f\"test_{i}.csv\"\n",
    "        train_df.to_csv(os.path.join(splits_dir, train_filename), index=False)\n",
    "        test_df.to_csv(os.path.join(splits_dir, test_filename), index=False)\n",
    "        print(f\"Saved {train_filename} and {test_filename}\")\n",
    "\n",
    "\n",
    "def log1p_normalization(arr):\n",
    "    \"\"\"  Apply log1p normalization to the given array \"\"\"\n",
    "\n",
    "    scale_factor = 100\n",
    "    return np.log1p((arr / np.sum(arr, axis=1, keepdims=True)) * scale_factor)\n",
    "\n",
    "\n",
    "def normalize_adata(adata: sc.AnnData) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    Normalize and apply log1p transformation to the expression matrix of an AnnData object.\n",
    "    (The function normalizes the gene expression by row)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : sc.AnnData\n",
    "        AnnData object containing gene expression data.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_adata = adata.copy()\n",
    "    filtered_adata.X = filtered_adata.X.astype(np.float64)\n",
    "    filtered_adata.X = log1p_normalization(filtered_adata.X)\n",
    "\n",
    "    return filtered_adata\n",
    "\n",
    "\n",
    "def load_adata(expr_path, genes=None, barcodes=None, normalize=False):\n",
    "    \"\"\"\n",
    "    Load AnnData object from a given path\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    expr_path : str\n",
    "        Path to the .h5ad file containing the AnnData object.\n",
    "    genes : list, optional\n",
    "        List of genes to retain. If None, all genes are kept.\n",
    "    barcodes : list, optional\n",
    "        List of barcodes (cells) to retain. If None, all cells are kept.\n",
    "    normalize : bool, optional\n",
    "        Whether to apply normalization (log1p normalization) to the data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Gene expression data as a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    adata = sc.read_h5ad(expr_path)\n",
    "    if barcodes is not None:\n",
    "        adata = adata[barcodes]\n",
    "    if genes is not None:\n",
    "        adata = adata[:, genes]\n",
    "    if normalize:\n",
    "        adata = normalize_adata(adata)\n",
    "    return adata.to_df()\n",
    "\n",
    "\n",
    "def merge_dict(main_dict, new_dict, value_fn=None):\n",
    "    \"\"\"\n",
    "    Merge new_dict into main_dict. If a key exists in both dicts, the values are appended.\n",
    "    Else, the key-value pair is added.\n",
    "    Expects value to be an array or list - if not, it is converted to a list.\n",
    "    If value_fn is not None, it is applied to each item in each value in new_dict before merging.\n",
    "    Args:\n",
    "        main_dict: main dict\n",
    "        new_dict: new dict\n",
    "        value_fn: function to apply to each item in each value in new_dict before merging\n",
    "    \"\"\"\n",
    "\n",
    "    if value_fn is None:\n",
    "        def value_fn(x): return x\n",
    "\n",
    "    for key, value in new_dict.items():\n",
    "        if not isinstance(value, list):\n",
    "            value = [value]\n",
    "        value = [value_fn(v) for v in value]\n",
    "        if key in main_dict:\n",
    "            main_dict[key] = main_dict[key] + value\n",
    "        else:\n",
    "            main_dict[key] = value\n",
    "\n",
    "    return main_dict\n",
    "\n",
    "\n",
    "def post_collate_fn(batch):\n",
    "    \"\"\" Post collate function to clean up batch \"\"\"\n",
    "\n",
    "    if batch[\"imgs\"].dim() == 5:\n",
    "        assert batch[\"imgs\"].size(0) == 1\n",
    "        batch[\"imgs\"] = batch[\"imgs\"].squeeze(0)\n",
    "\n",
    "    if batch[\"coords\"].dim() == 3:\n",
    "        assert batch[\"coords\"].size(0) == 1\n",
    "        batch[\"coords\"] = batch[\"coords\"].squeeze(0)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def merge_fold_results(arr):\n",
    "    \"\"\" Merges results from multiple cross-validation folds, aggregating Pearson correlation across all folds. \"\"\"\n",
    "    aggr_dict = {}\n",
    "    for dict in arr:\n",
    "        for item in dict['pearson_corrs']:\n",
    "            gene_name = item['name']\n",
    "            correlation = item['pearson_corr']\n",
    "            aggr_dict[gene_name] = aggr_dict.get(gene_name, []) + [correlation]\n",
    "\n",
    "    aggr_results = []\n",
    "    all_corrs = []\n",
    "    for key, value in aggr_dict.items():\n",
    "        aggr_results.append({\n",
    "            \"name\": key,\n",
    "            \"pearson_corrs\": value,\n",
    "            \"mean\": np.mean(value),\n",
    "            \"std\": np.std(value)\n",
    "        })\n",
    "        all_corrs += value\n",
    "\n",
    "    mean_per_split = [d['pearson_mean'] for d in arr]\n",
    "\n",
    "    return {\n",
    "        \"pearson_corrs\": aggr_results,\n",
    "        \"pearson_mean\": np.mean(mean_per_split),\n",
    "        \"pearson_std\": np.std(mean_per_split),\n",
    "        \"mean_per_split\": mean_per_split\n",
    "    }\n",
    "\n",
    "\n",
    "class InferenceEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract base class for building inference encoders.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    weights_path : str or None\n",
    "        Path to the model weights (optional).\n",
    "    model : torch.nn.Module\n",
    "        The model architecture.\n",
    "    eval_transforms : callable\n",
    "        Evaluation transformations applied to the input images.\n",
    "    precision : torch.dtype\n",
    "        The data type of the model's parameters and inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights_path=None, **build_kwargs):\n",
    "        super(InferenceEncoder, self).__init__()\n",
    "\n",
    "        self.weights_path = weights_path\n",
    "        self.model, self.eval_transforms, self.precision = self._build(weights_path, **build_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build(self, **build_kwargs):\n",
    "        pass\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def get_eval_transforms(mean, std):\n",
    "    \"\"\"\n",
    "    Creates the evaluation transformations for preprocessing images. This includes\n",
    "    converting the images to tensor format and normalizing them with given mean and std.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean : list\n",
    "        The mean values used for normalization.\n",
    "    std : list\n",
    "        The standard deviation values used for normalization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    transforms.Compose\n",
    "        A composed transformation function that applies the transformations in sequence.\n",
    "    \"\"\"\n",
    "    trsforms = []\n",
    "\n",
    "    # Convert image to tensor\n",
    "    trsforms.append(lambda img: TF.to_tensor(img))\n",
    "\n",
    "    if mean is not None and std is not None:\n",
    "        # Normalize the image\n",
    "        trsforms.append(lambda img: TF.normalize(img, mean, std))\n",
    "\n",
    "    return transforms.Compose(trsforms)\n",
    "\n",
    "class ResNet50InferenceEncoder(InferenceEncoder):\n",
    "    \"\"\"\n",
    "    A specific implementation of the InferenceEncoder class for ResNet50.\n",
    "    This encoder is used to extract features from images using a pretrained ResNet50 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def _build(\n",
    "        self,\n",
    "        weights_root=\"resnet50.tv_in1k\",\n",
    "        timm_kwargs={\"features_only\": True, \"out_indices\": [3], \"num_classes\": 0},\n",
    "        pool=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build the ResNet50 model and load its weights. It supports both pretrained models\n",
    "        from the internet and pretrained models from a given weights path (offline).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        weights_root : str\n",
    "            Path to pretrained model weights. Defaults to \"resnet50.tv_in1k\" (if online).\n",
    "        timm_kwargs : dict\n",
    "            Additional arguments for creating the ResNet50 model via the timm library.\n",
    "        pool : bool\n",
    "            Whether to apply adaptive average pooling to the output of the model. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            A tuple containing the ResNet50 model, the evaluation transformations, and the precision type.\n",
    "        \"\"\"\n",
    "\n",
    "        if weights_root == \"resnet50.tv_in1k\":\n",
    "            pretrained = True\n",
    "            print(\"Load pretrained Resnet50 from internet\")\n",
    "        else:\n",
    "            pretrained = False\n",
    "            print(f\"Load pretrained Resnet50 offline from weights path: {weights_root}\")\n",
    "\n",
    "        # Build the model using the timm library\n",
    "        model = timm.create_model(\"resnet50.tv_in1k\", pretrained=pretrained, **timm_kwargs)\n",
    "\n",
    "        # If not using a pretrained model, load weights from the specified path\n",
    "        if not pretrained and os.path.exists(weights_root):\n",
    "            # Load the weights\n",
    "            checkpoint = torch.load(weights_root, map_location='cpu', weights_only=True)  # or 'cuda' if using GPU\n",
    "\n",
    "            # Remove the classifier layers from the checkpoint\n",
    "            model_state_dict = model.state_dict()\n",
    "            checkpoint = {k: v for k, v in checkpoint.items() if k in model_state_dict}\n",
    "\n",
    "            # Load the weights into the model\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "        elif not pretrained:\n",
    "            # Issue a warning if the weights file is missing\n",
    "            print(f\"\\n!!! WARNING: The specified weights file '{weights_root}' does not exist. The model will be initialized with random weights.\\n\")\n",
    "\n",
    "        imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        eval_transform = get_eval_transforms(imagenet_mean, imagenet_std)\n",
    "        precision = torch.float32\n",
    "        if pool:\n",
    "            self.pool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        else:\n",
    "            self.pool = None\n",
    "\n",
    "        return model, eval_transform, precision\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.forward_features(x)\n",
    "        if self.pool:\n",
    "            out = self.pool(out).squeeze(-1).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        out = self.model(x)\n",
    "        if isinstance(out, list):\n",
    "            assert len(out) == 1\n",
    "            out = out[0]\n",
    "        return out\n",
    "\n",
    "def inf_encoder_factory(enc_name):\n",
    "    \"\"\"\n",
    "    Factory function to instantiate an encoder based on the specified name.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    enc_name : str\n",
    "        The name of the encoder model to instantiate (e.g., 'resnet50').\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    class\n",
    "        The encoder class corresponding to the specified encoder name.\n",
    "    \"\"\"\n",
    "\n",
    "    if enc_name == 'resnet50':\n",
    "        return ResNet50InferenceEncoder\n",
    "\n",
    "    raise ValueError(f\"Unknown encoder name {enc_name}\")\n",
    "\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to read ST + H&E from an HDF5 (.h5) file\n",
    "    The dataset loads images and their associated barcodes/cells and coordinates in chunks for efficient data handling.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing the images, barcodes, and coordinates.\n",
    "    img_transform : callable, optional\n",
    "        A transformation function to apply to the images. Defaults to None.\n",
    "    chunk_size : int, optional\n",
    "        Number of items to load per batch. Defaults to 1000.\n",
    "    n_chunks : int\n",
    "        The total number of chunks, calculated based on the size of the 'barcode' array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5_path, img_transform=None, chunk_size=1000):\n",
    "        self.h5_path = h5_path\n",
    "        self.img_transform = img_transform\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            self.n_chunks = int(np.ceil(len(f['barcode']) / chunk_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_chunks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a batch of data (images, barcodes, and coordinates) from the HDF5 file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            The index of the chunk to fetch.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the images, barcodes, and coordinates for the specified chunk.\n",
    "        \"\"\"\n",
    "\n",
    "        start_idx = idx * self.chunk_size\n",
    "        end_idx = (idx + 1) * self.chunk_size\n",
    "        # Open the HDF5 file and load the specific chunk of data\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            imgs = f['img'][start_idx:end_idx]\n",
    "            barcodes = f['barcode'][start_idx:end_idx].flatten().tolist()\n",
    "            coords = f['coords'][start_idx:end_idx]\n",
    "\n",
    "        # Apply image transformations if any (e.g. to Torch and normalization)\n",
    "        if self.img_transform:\n",
    "            imgs = torch.stack([self.img_transform(img) for img in imgs])\n",
    "\n",
    "        return {'imgs': imgs, 'barcodes': barcodes, 'coords': coords}\n",
    "\n",
    "\n",
    "def embed_tiles(dataloader, model: torch.nn.Module, embedding_save_path: str, device: str, precision):\n",
    "    \"\"\"\n",
    "    Extracts embeddings from image tiles using the specified model and saves them to an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader providing the batches of image tiles.\n",
    "    model : torch.nn.Module\n",
    "        The model used to generate embeddings from the tiles.\n",
    "    embedding_save_path : str\n",
    "        Path where the generated embeddings will be saved.\n",
    "    device : str\n",
    "        The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "    precision : torch.dtype\n",
    "        The precision (data type) to use for inference (e.g., float16 for mixed precision).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    # Iterate over the batches in the DataLoader\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch = post_collate_fn(batch)\n",
    "        imgs = batch['imgs'].to(device).float()\n",
    "        # Apply model on images\n",
    "        with torch.inference_mode():\n",
    "            if torch.cuda.is_available():  # Use mixed precision only if CUDA is available\n",
    "                with torch.amp.autocast('cuda', dtype=precision):\n",
    "                    embeddings = model(imgs)\n",
    "            else:  # No mixed precision on CPU\n",
    "                embeddings = model(imgs)\n",
    "\n",
    "        # Set mode to 'w' for the first batch, 'a' for appending subsequent batches\n",
    "        mode = 'w' if batch_idx == 0 else 'a'\n",
    "\n",
    "        # Create a dictionary with embeddings and other relevant data to save\n",
    "        asset_dict = {'embeddings': embeddings.cpu().numpy()}\n",
    "        asset_dict.update({key: np.array(val) for key, val in batch.items() if key != 'imgs'})\n",
    "\n",
    "        # Save the embeddings to the HDF5 file\n",
    "        save_hdf5(embedding_save_path, asset_dict=asset_dict, mode=mode)\n",
    "\n",
    "    return embedding_save_path\n",
    "\n",
    "\n",
    "def generate_embeddings(embed_path, encoder, device, tile_h5_path, batch_size, num_workers, overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate embeddings for images and save to a specified path.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_path : str\n",
    "        Path to save the embeddings.\n",
    "    encoder : torch.nn.Module\n",
    "        The encoder model for generating embeddings.\n",
    "    device : torch.device\n",
    "        Device to use for computation (e.g., 'cuda' or 'cpu').\n",
    "    tile_h5_path : str\n",
    "        Path to the HDF5 file containing images.\n",
    "    batch_size : int\n",
    "        Batch size for the DataLoader.\n",
    "    num_workers : int\n",
    "        Number of worker threads for data loading.\n",
    "    overwrite : bool, optional\n",
    "        If True, overwrite existing embeddings. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the embeddings file doesn't exist or overwrite is True, proceed to generate embeddings\n",
    "    if not os.path.isfile(embed_path) or overwrite:\n",
    "        print(f\"Generating embeddings for {embed_path} ...\")\n",
    "\n",
    "        # Set encoder to evaluation mode and move it to the device\n",
    "        encoder.eval()\n",
    "        encoder.to(device)\n",
    "\n",
    "        # Create dataset and dataloader for tiles\n",
    "        tile_dataset = H5Dataset(tile_h5_path, chunk_size=batch_size, img_transform=encoder.eval_transforms)\n",
    "        tile_dataloader = torch.utils.data.DataLoader(\n",
    "            tile_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        # Generate and save embeddings\n",
    "        embed_tiles(tile_dataloader, encoder, embed_path, device, encoder.precision)\n",
    "    else:\n",
    "        print(f\"Skipping embedding {os.path.basename(embed_path)} as it already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DOES NOT NEED TRAINING\n",
    "def train(\n",
    "    train_directory_path: str\n",
    "):\n",
    "    # no train\n",
    "    print(train_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_predict(X_train, y_train, X_test, k=200):\n",
    "    \"\"\"Find k nearest neighbors in training set for each test point and return average of their y values\"\"\"\n",
    "    predictions = []\n",
    "    for i, test_embedding in enumerate(X_test):\n",
    "        # Calculate distances to all training points\n",
    "        distances = np.sqrt(np.sum((X_train - test_embedding) ** 2, axis=1))\n",
    "        \n",
    "        # If the test point is in the training set, exclude it\n",
    "        if X_test is X_train:\n",
    "            distances[i] = np.inf\n",
    "            \n",
    "        # Find k nearest neighbors\n",
    "        nn_indices = np.argpartition(distances, k)[:k]\n",
    "        # Use average of their gene expressions as prediction\n",
    "        predictions.append(np.mean(y_train[nn_indices], axis=0))\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    data_file_path: str,  # Path to a test dataset (in Zarr format) to perform inference on.\n",
    "    model_directory_path: str  # Path to save the trained model and results\n",
    "):\n",
    "\n",
    "    # Load the NPZ file\n",
    "    embeddings_table = np.load(os.path.join(model_directory_path, f\"combined_dataset.npz\"))\n",
    "\n",
    "    # Load and normalize the data\n",
    "    embeddings = embeddings_table['embeddings'][::100]\n",
    "    gene_expression = embeddings_table['gene_expression'][::100]\n",
    "    gene_expression_normalized = log1p_normalization(gene_expression)\n",
    "\n",
    "    # Load training configuration parameters\n",
    "    config_path = os.path.join(model_directory_path, \"config.json\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        args_dict = json.load(f)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "\n",
    "\n",
    "    # Set device to GPU if available, else use CPU!!\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ### Preprocess and Embedding Data + Regression inference ###\n",
    "\n",
    "    # Read the spatial data from the provided file\n",
    "    sdata = sd.read_zarr(data_file_path)\n",
    "\n",
    "    # Extract cell IDs for test and validation groups\n",
    "    cell_ids = list(sdata[\"cell_id-group\"].obs.query(\"group == 'test' or group == 'validation'\")[\"cell_id\"])\n",
    "\n",
    "    # Extract gene names from the spatial data\n",
    "    gene_names = list(sdata[\"anucleus\"].var.index)\n",
    "\n",
    "    # Extract the name of the dataset from the file path (without extension)\n",
    "    name_data = os.path.splitext(os.path.basename(data_file_path))[0]\n",
    "\n",
    "    # Directory for processed test dataset (temporary storage)\n",
    "    dir_processed_dataset_test = os.path.join(\"/tmp\", f\"processed_dataset_test\")\n",
    "    os.makedirs(dir_processed_dataset_test, exist_ok=True)\n",
    "\n",
    "    # Directory to store the test data embeddings (temporary storage)\n",
    "    test_embed_dir = os.path.join(dir_processed_dataset_test, \"ST_data_emb\")\n",
    "    os.makedirs(test_embed_dir, exist_ok=True)\n",
    "\n",
    "    # Preprocess the test data for embedding (patch extraction)\n",
    "    preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_ids, dir_processed_dataset_test,\n",
    "                                                 args.target_patch_size, args.vis_width, args.show_extracted_images)\n",
    "\n",
    "    print(f\"\\n-- {name_data} EMBEDDING--\\n\")\n",
    "    # Generate and load the embeddings for the test data\n",
    "    assets = embedding_and_load_data(name_data, dir_processed_dataset_test, test_embed_dir, args, device)\n",
    "\n",
    "    # Extract embeddings features for prediction\n",
    "    X_test = assets[\"embeddings\"]\n",
    "    print(\"Embedding shape (X_test):\", X_test.shape)\n",
    "\n",
    "    average_predictions = nearest_neighbor_predict(embeddings, gene_expression_normalized, X_test)\n",
    "\n",
    "    ### Prepare and Return Predictions ###\n",
    "\n",
    "    # Convert the predictions to a DataFrame (the gene expression value must be rounded to two decimal places)\n",
    "    prediction = pd.DataFrame(np.round(average_predictions, 2), index=cell_ids, columns=gene_names)\n",
    "    # Reset index to have 'cell_id' as a column\n",
    "    prediction = prediction.reset_index(names=\"cell_id\")\n",
    "\n",
    "    # Melt the DataFrame to the expected output for the challenge\n",
    "    prediction = prediction.melt(id_vars=\"cell_id\", var_name=\"gene\", value_name=\"prediction\")\n",
    "    # prediction = prediction.sort_values(by=[\"cell_id\", \"gene\"]).reset_index(drop=True)\n",
    "\n",
    "    # Free memory by deleting large variables and performing garbage collection\n",
    "    del average_predictions, sdata, X_test, assets\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n-- {name_data} PREDICTION DONE\\n\")\n",
    "\n",
    "    # Return the final prediction DataFrame\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunch.test(\n",
    "    no_determinism_check=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
